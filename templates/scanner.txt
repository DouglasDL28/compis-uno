import time
from typing import Tuple

from automata.FiniteAutomata import FiniteAutomata

class Token():
    def __init__(self, name, lexeme) -> None:
        self.name: str = name
        self.lexeme: str = lexeme

    def __str__(self) -> str:
        return f"<{self.name}, '{self.lexeme}'>"

class Scanner(FiniteAutomata):
    """ Deterministic Finite Automata. Inherits from FiniteAutomata. """

    def __init__(self,) -> None:
        self.Q = {{ Q }}
        self.Sigma = {{ Sigma }}
        self.delta = {{ delta }}
        self.q_init = {{ q_init }}
        self.F = {{ F }}
        self.token_names = {{ token_names }}
        self.token_states = {{ token_states }}
        self.keywords = {{ keywords }}
        self.except_keyword = {{ except_keyword }}
        self.ignore = {{ ignore }}

    def get_keywords_set(self):
        kwrds = set()
        for kwrd in self.keywords:
            lexeme = ''.join(chr(c) for c in self.keywords[kwrd])
            kwrds.add(lexeme)

        return kwrds

    def move(self, s, c):
        """ Move from state s with character c defined in âˆ‚. """

        try:
            return self.delta[s][c]
        except:
            return None

    def simulate(self, filename=None) -> Tuple[bool, float]:

        # TODO defensive programming
        file = open(filename, 'r').read()
        file_len = len(file)

        tokens = []

        start_t = time.time()

        current_token = None
        lexeme = ""

        s = self.q_init
        i = 0

        while (i <= file_len - 1):
            
            # IGNORE SET
            if ord(file[i]) in self.ignore:
                i += 1
                continue

            s = self.move(s, ord(file[i]))
        
            if s in self.F: # terminal
                lexeme += file[i]

                # find first token that matches
                for j, states in enumerate(self.token_states):
                    if s in states:
                        token = self.token_names[j]
                        if (token in self.except_keyword
                            and lexeme in self.get_keywords_set()):
                            current_token = Token(lexeme, lexeme) # keyword
                        else:
                            current_token = Token(token, lexeme) # token

                        last_idx = i

                        break

                if i == (file_len - 1): # eof
                    tokens.append(current_token)
                    # print(current_token)
                    current_token = None

                elif not self.move(s, ord(file[i+1])): # no transition
                    lexeme = ""
                    tokens.append(current_token)
                    # print(current_token)
                    current_token = None
                    s = self.q_init # restart dfa
                
                i += 1

            else: # no terminal
                # no transition or eof
                if i == (file_len - 1) or not self.move(s, ord(file[i+1])):
                    print("Lexical error.", file[i])
                    if current_token:
                        tokens.append(current_token)
                        # print(current_token)
                        current_token = None
                    lexeme = "" # empty buffer
                    s = self.q_init # restart dfa
                    i = last_idx + 1

                else: 
                    lexeme += file[i]
                    i += 1

        end_t = time.time()
        return True, tokens, (end_t - start_t)


if __name__ == "__main__":
    dfa = Scanner()

    filename = input("Input filename: ") or "tests/scan.txt"

    sim, tokens, elapsed_t  = dfa.simulate(filename=filename)
    print(f"Direct Construction DFA simulation: {sim} -- elapsed time: {elapsed_t}\n")

    print(*tokens, sep='\n')

